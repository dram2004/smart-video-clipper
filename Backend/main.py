#Backend/main.py
import os
import io
import re 
import asyncio
import uuid 
import json
import base64
import httpx
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
# imports needed for chunking audio into segments
from pydub import AudioSegment
import static_ffmpeg
static_ffmpeg.add_paths() # tells pydub where to find ffmpeg binary

# imports for PDF output logic
from fpdf import FPDF
from fastapi.responses import Response

# imports for pdf processing
import fitz

# imports for pratice content gen
from pydantic import BaseModel
from typing import List, Optional

# --- Load Required Environment Variables ---

load_dotenv()
# get cloudflare account ID
cf_acc_id = os.getenv("CLOUDFLARE_ACCOUNT_ID")
# get cloudflare API token
cf_api_token = os.getenv("CLOUDFLARE_API_TOKEN")
# get cloudflare d1 DB ID
cf_d1_ID = os.getenv("CLOUDFLARE_D1_ID")
#  get cloudflare Vector index
cf_vector_index = os.getenv("CLOUDFLARE_VECTOR_INDEX")

# --- Verify the Environment Variables Exist
if not all([cf_acc_id,cf_api_token,cf_d1_ID,cf_vector_index]):
    # if either of the keys do not exist, raise a runtime error
    raise RuntimeError("Missing Cloudflare credentials in .env file...")

# --- Initialize the App
app = FastAPI(title="Smart Video Clipper API")

# --- Setup CORS --> allows Next.js Frontend to communicate with our Backend ---
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], # replace with frontend URL when ready
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"]
)

# --- Setup Cloudflare Configuration ---

# AI base url first to use when setting up httpx asynchronous client
AI_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/ai/run"
# vector base URL
VECTOR_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/vectorize/v2/indexes/{cf_vector_index}/insert"
# D1 database base url
D1_BASE = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/d1/database/{cf_d1_ID}/query"
# headers containing API token for asynchronous client setup
HEADERS = {"Authorization": f"Bearer {cf_api_token}"}

# --- DATA MODELS ---
class ExamRequest(BaseModel):
    homework_id: int

class ExamProblem(BaseModel):
    question: str
    solution: str
    explanation: str

# --- HELPERS ---
# Helper 1: Transcribing audio using openai/whisper model
# Helper 1: Transcribing audio using openai/whisper model
async def call_whisper(audio_bytes: bytes): 
    """
    Sends audio as MULTIPART FORM DATA to force 'language=en'.
    This prevents the model from crashing (Code 3010) when it hears silence/noise.
    """
    model = "@cf/openai/whisper"
    
    # We do NOT manually set Content-Type here; httpx handles the boundary automatically
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers={"Authorization": f"Bearer {cf_api_token}"}, 
                files={"audio_file": ("chunk.mp3", audio_bytes, "audio/mpeg")},
                data={"language": "en"}, # <--- THIS IS THE KEY FIX
                timeout=60.0 
            )

            if response.status_code != 200:
                print(f"Whisper Error (Chunk failed): {response.text}")
                return "" 
            
            result = response.json()
            return result.get("result", {}).get("text", "")

        except Exception as e:
            print(f"Whisper Exception: {e}")
            return ""

# Helper 2: call LLM with transcript to get a structured summary of transcript using meta/llama-3-8b-instruct model
async def call_llm(text_chunks: list): # function takes in string transcript
    """
    function sends transcript generated by call_whisper and calls llama-3 to generate structured study notes
    1. MAP: summarizes each five minute chunk individually
    2. REDUCE: combines all summaries into final formatted notes
    """
    # define model --> using 8b llama-3.1
    model = "@cf/meta/llama-3.1-8b-instruct"

    print(f"DEBUG: Starting Map-Reduce on {len(text_chunks)} chunks...")
    
    # STEP 1: MAP --> summarize each chunk in parallel
    async def summarize_text_chunk(text):
        if (len(text) < 50): 
            return ""
        # write prompt for single chunk processing
        prompt = f"""
        Summarize the key technical concepts and announcements in this specific lecture segment.
        Keep it concise in a range of 3-5 sentences.
        TRANSCRIPT SEGMENT:
        {text}
        """
        # define payload including role, prompt, and max number of tokens
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 500
        }
        # call the model with AsyncClient
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{AI_base_url}/{model}", 
                    headers=HEADERS,
                    json=payload,
                    timeout=30.0
                )
                return response.json().get("result", {}).get("response", "")
            except Exception as e:
                print(f"Chunk processing error: {e}")
                return ""

    # call model on all text chunks at the same time
    mini_summaries = await asyncio.gather(*[summarize_text_chunk(chunk) for chunk in text_chunks])

    # combine summaries of all chunks into "master context" to send back to model
    combined_summaries = "\n".join(filter(None, mini_summaries)) # join all mini summaries seperated by newline

    print(f"Combined summary length from all text chunks: {len(combined_summaries)} characters")

    # STEP 2: REDUCE
    # prompt engineering:
    final_prompt = f"""
    You are an expert student assistant. 
    Below are summaries from different parts of a lecture (in chronological order).
    Compile them into one cohesive set of notes.

    OUTPUT FORMAT:
    1. **Announcements**: Any logistics/deadlines mentioned.
    2. **Summary**: A cohesive narrative of the lecture topics.
    3. **Key Points**: Bullet points of specific concepts/theorems.

    LECTURE DATA:
    {combined_summaries}
    """
    # prepare payload: truncate transcript to ~6000 characters bc context limits of free plan
    # in future, can use "map_reduce" strategy for long transcripts
    payload = {
        "messages": [
            {"role": "system", "content": "You are a valuable study assistant"},
            {"role": "user", "content": final_prompt},
        ],
        "max_tokens": 2048
    }
    # call model again with the final prompt built using the summaries of all the text chunks 
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers=HEADERS,
                json=payload,
                timeout=60.0
            )

            # --- THE DEBUG BLOCK ---
            print(f"DEBUG: Status Code: {response.status_code}")
            print(f"DEBUG: Raw Response: {response.text}") 
            # -----------------------

            # make sure error didnt occur, if it did raise exception
            if response.status_code != 200:
                print(f"LLM Error: {response.text}")
                return "Error generating summary of transcript using LLM"
            
            # get response json
            result = response.json()
            if "result" in result and "response" in result["result"]:
                return result["result"]["response"]
        
            # Fallback if AI fails
            print(f"DEBUG: Final Reduction Failed. Response: {result}")
            return combined_summaries # Return the raw summaries so the user at least gets something

        except Exception as e:
            print(f"DEBUG: Exception occurred: {e}")
            return f"Error: {str(e)}"

# Helper 3: need function to convert list of text strings into vectors for RAG
async def generate_embeddings(text_chunks):
    """
    Converts list of text strings into vectors.
    """
    model = "@cf/baai/bge-base-en-v1.5"

    print(f"   --> Debug: Requesting embeddings for {len(text_chunks)} chunk(s)...")

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers=HEADERS,
                json={"text": text_chunks}
            )
            
            # DEBUG: Print if it fails
            if response.status_code != 200:
                print(f"   ❌ Cloudflare AI Error: {response.status_code}")
                print(f"   ❌ Response Body: {response.text}")
                return []

            data = response.json()
            
            # Check if 'result' exists
            if not data.get("result"):
                print(f"   ❌ AI returned success but no result: {data}")
                return []

            return data["result"].get("data", [])

        except Exception as e:
            print(f"   ❌ Embedding Exception: {e}")
            return []

# Helper 4: need helper for generating embeddings for each chunk then saving it to vectorize with proper metadata
async def save_vectors(filename, text_chunks):
    if not cf_vector_index: return
    print("generating embeddings...")

    embeddings_data = await generate_embeddings(text_chunks)
    vectors = []
    chunk_duration_seconds = 300 

    for i, embedding_obj in enumerate(embeddings_data):
        start_time = i * chunk_duration_seconds 
        
        # Create a safe ID (spaces are okay, but let's be robust)
        safe_id = f"{filename.replace(' ', '_')}_chunk_{i}_{uuid.uuid4().hex[:6]}"
        
        # CRITICAL FIX: Remove newlines from text_preview
        raw_text = text_chunks[i]
        clean_preview = raw_text.replace("\n", " ").replace("\r", " ")[:1000] # <--- 1000 chars & No Newlines!

        vectors.append({
            "id": safe_id,
            "values": embedding_obj, 
            "metadata": {
                "filename": filename,
                "text_preview": clean_preview, 
                "chunk_index": i,
                "start_time_sec": start_time
            }
        })

    # NDJSON requires exactly one JSON object per line
    ndjson_payload = "\n".join([json.dumps(v) for v in vectors])

    async with httpx.AsyncClient() as client:
        response = await client.post(
            VECTOR_base_url,
            headers={"Authorization": f"Bearer {cf_api_token}", "Content-Type": "application/x-ndjson"},
            content=ndjson_payload,
            timeout=60.0
        )
        if response.status_code == 200:
            print(f"✅ Indexed {len(vectors)} vectors in Cloudflare!")
        else:
            print(f"❌ Vector Error: {response.text}")

# Helper 5: need helper for saving filename, transcript, and summary in database
async def save_2_db(filename, transcript, summary):
    """
    save metadata to SQL D1
    necessary for remembering all lectures passed into smart clipper
    """
    sql_command = "INSERT INTO lectures (filename, transcript, summary) VALUES (?, ?, ?)"

    # define payload with sql command and provided arguments
    payload = {"sql": sql_command, "params": [filename, transcript, summary]}

    # send to D1 using asyncclient
    async with httpx.AsyncClient() as client:
        response = await client.post(D1_BASE, headers=HEADERS, json=payload)
        
        data = response.json()
        
        if response.status_code == 200 and data.get("success"):
            print("Succesfully saved record to D1 SQL")
        else:
            print(f"❌ DATABASE ERROR: {response.status_code}")
            print(data)

# Helper 6: Convert LaTeX to HTML for FPDF2 (Safe Latin-1 Version)
def clean_for_pdf(text: str) -> str:
    """
    Converts LaTeX math into simple HTML.
    CRITICAL CHANGE: Maps Greek/Symbols to plain text names to avoid 
    Unicode errors with standard PDF fonts.
    """
    if not text: return ""
    
    # 1. HTML Entity Map (Mapped to Latin-1 safe text)
    symbol_map = {
        # Greek Letters -> Text names
        r"\\phi": "phi",
        r"\\Phi": "Phi",
        r"\\theta": "theta",
        r"\\Theta": "Theta",
        r"\\alpha": "alpha",
        r"\\beta": "beta",
        r"\\gamma": "gamma",
        r"\\lambda": "lambda",
        r"\\pi": "pi",
        r"\\sigma": "sigma",
        r"\\infty": "infinity",
        
        # Operators -> Standard chars
        r"\\le": "<=",
        r"\\ge": ">=",
        r"\\neq": "!=",
        r"\\equiv": "=",
        r"\\approx": "~=",
        r"\\times": "x",
        r"\\cdot": "*",
        
        # Sets -> Bold Text
        r"\\mathbb\{Z\}": "<b>Z</b>",
        r"\\mathbb\{R\}": "<b>R</b>",
        r"\\mathbb\{N\}": "<b>N</b>",
        r"\\mathbb\{F\}": "<b>F</b>",
        r"\\mathbb\{Q\}": "<b>Q</b>",
        
        # Misc
        r"\\pmod\{(.+?)\}" : r" (mod \1)",
    }

    clean = text
    for latex, html in symbol_map.items():
        clean = re.sub(latex, html, clean)

    # 2. Regex for Superscripts (x^2 or x^{12}) -> x<sup>2</sup>
    # This works fine because numbers are Latin-1 safe!
    clean = re.sub(r"\^\{([^{}]+)\}", r"<sup>\1</sup>", clean) 
    clean = re.sub(r"\^([0-9a-zA-Z])", r"<sup>\1</sup>", clean) 

    # 3. Regex for Subscripts (x_i) -> x<sub>i</sub>
    clean = re.sub(r"_\{([^{}]+)\}", r"<sub>\1</sub>", clean)
    clean = re.sub(r"_([0-9a-zA-Z])", r"<sub>\1</sub>", clean)

    # 4. Fractions: \frac{a}{b} -> (a)/(b)
    clean = re.sub(r"\\frac\{(.+?)\}\{(.+?)\}", r"(\1)&frasl;(\2)", clean)

    # 5. Formatting
    clean = re.sub(r"\\textbf\{(.+?)\}", r"<b>\1</b>", clean)
    clean = re.sub(r"\\textit\{(.+?)\}", r"<i>\1</i>", clean)
    
    # 6. Cleanup
    clean = clean.replace("$", "").replace("\\", "")

    return clean

# Helper 7: Call CF vision model to extract math problems from PDF as LaTeX
async def extract_problems_from_image(image_bytes):
    """
    Sends an image to Llama 3.2 Vision using the standard OpenAI 'image_url' format.
    """
    model = "@cf/meta/llama-3.2-11b-vision-instruct"

    # 1. Encode image to Base64
    base64_image = base64.b64encode(image_bytes).decode("utf-8")
    
    # 2. Create the Data URL (Crucial for this API)
    data_url = f"data:image/jpeg;base64,{base64_image}"

    prompt = """
    Analyze this homework page. Extract every math problem you see into a strictly formatted list.
    Rules:
    1) Output only the problem, separated by the delimiter "---Problem---".
    2) Use standard LaTeX formatting for all math equations (e.g. use $...$ or $$...$$).
    3) Do not solve the problems. Just transcribe them.
    4) Ignore headers, footers, or page numbers.
    """

    # 3. Construct Payload (OpenAI Format)
    payload = {
        "messages": [
            {
                "role": "user", 
                "content": [
                    {"type": "text", "text": prompt},
                    {
                        "type": "image_url", 
                        "image_url": {
                            "url": data_url
                        }
                    }
                ]
            }
        ],
        "max_tokens": 2048
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}", 
                headers=HEADERS,
                json=payload,
                timeout=60.0 
            )
            
            # Debugging logs
            if response.status_code != 200:
                print(f"❌ Vision API Error {response.status_code}: {response.text}")
                return []
            
            result = response.json()
            
            if "result" in result:
                inner = result["result"]
                # Robust Key Checking
                if isinstance(inner, dict):
                    raw_text = inner.get("response") or inner.get("description") or inner.get("text")
                else:
                    raw_text = str(inner)

                if raw_text:
                    problems = raw_text.split("---Problem---")
                    return [p.strip() for p in problems if p.strip()]
            
            return []

        except Exception as e:
            print(f"❌ Vision Exception: {e}")
            return []

# Helper 8: json sanitization
def sanitize_json_output(text: str) -> str:
    """
    Fixes common JSON errors from LLMs, specifically unescaped backslashes in LaTeX.
    Example: Converts "\frac" to "\\frac" so json.loads() doesn't crash.
    """
    # 1. Remove markdown code blocks if present
    text = text.replace("```json", "").replace("```", "").strip()
    
    # 2. Fix invalid escape sequences (The Magic Regex)
    # This looks for a backslash that is NOT followed by a valid JSON escape char (" \ / b f n r t u)
    # and doubles it.
    # Pattern: \ (negative lookahead for valid chars)
    text = re.sub(r'\\(?![/u"bfnrt\\])', r'\\\\', text)
    
    return text

# --- ENDPOINTS ---
@app.get("/") # for base path
def health_check():
    # base path operation function
    """checks to see if server is running"""
    return {"status": "running", "service": "AI-Powered Smart Video Clipper"}

@app.get("/lectures") # endpoint for frontend to call to query database on which lectures are currently stored
async def get_lectures():
    """
    Path operation function responsible for fetching all saved lectures from D1 DB
    """
    # write SQL query
    sql = "SELECT * FROM lectures ORDER BY upload_date DESC" # gets all lectures, ordered in descending order based on the date they got uploaded

    # query d1 using httpx.AsyncClient
    async with httpx.AsyncClient() as client:
        response = await client.post(
            D1_BASE,
            headers=HEADERS,
            json={"sql": sql, "params": []}
        )
    
        # parse D1 response structure
        data = response.json() # get json received from D1
        if data.get("success") and data.get("result"):
            # if d1 returned success and result, return query rows
            return data["result"][0].get("results", [])
        # if did not get successful response, return nothing
        return []
 
@app.get("/search")
async def search_lectures(q: str):
    if not q: return []
    print(f"\n--- DEBUG SEARCH START: '{q}' ---")

    # 1. Generate Embedding
    print("Step 1: Generating embedding...")
    try:
        query_vector_data = await generate_embeddings([q])
    except Exception as e:
        print(f"❌ Error generating embedding: {e}")
        return []

    if not query_vector_data: 
        print("❌ Embedding generation returned empty data!")
        return []
    
    query_vector = query_vector_data[0]
    print(f"✅ Embedding generated (Length: {len(query_vector)})")

    # 2. Prepare Vectorize Request
    search_url = VECTOR_base_url.replace("/insert", "/query")
    payload = {
        "vector": query_vector,
        "topK": 5, 
        "returnMetadata": "all"
    }
    
    print(f"Step 2: Sending query to Cloudflare Vectorize...")
    print(f"Target URL: {search_url}")

    # 3. Send Request
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(search_url, headers=HEADERS, json=payload)
            
            # Print raw response details
            print(f"Step 3: Received Response. Status Code: {response.status_code}")
            print(f"Raw Response Body: {response.text}")

            data = response.json()
            
            matches = []
            if data.get("result"):
                raw_matches = data["result"].get("matches", [])
                print(f"✅ Found {len(raw_matches)} raw matches from Cloudflare.")
                
                for match in raw_matches:
                    score = match.get("score", 0)
                    filename = match.get("metadata", {}).get("filename", "Unknown")
                    print(f" -- Candidate: {filename} | Score: {score}")

                    matches.append({
                        "score": score,
                        "text": match["metadata"].get("text_preview", "No text"),
                        "filename": filename,
                        "timestamp": match["metadata"].get("start_time_sec", 0)
                    })
            else:
                print("⚠️ Cloudflare returned 'success' but no 'result' object.")

            return matches

        except Exception as e:
            print(f"❌ Search Exception: {e}")
            return []

@app.post("/test-ai") # for post requests to /test-ai
async def test_ai_connection():
    # path operation function calling cf model to verify that cf credentials work
    model = "@cf/meta/llama-3-8b-instruct"
    payload = { # what is sent to the model --> just tells it to say hello as a test that it works
        "messages": [
            {"role": "user", "content": "Say 'Hello from Cloudflare' and nothing else."}
        ]
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}", 
                headers=HEADERS,
                json=payload,
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-lecture") # for post requests to /process-lecture for audio transcription + llm summary
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Upload an MP3/WAV file --> pass to Whisper --> Returns text transcript back

    Send text transcipt to LLM --> Llama-3 returns summary of lecture
    """
    print(f"Processing {file.filename}...")

    clean_filename = os.path.splitext(file.filename)[0]

    # read the uploaded file into memory
    audio_data = await file.read()

    # in order to process long mp3's with whisper, need to chunk video into shorter segments
    audio = AudioSegment.from_file(io.BytesIO(audio_data)) # creates audio segment object from audio bytes

    # define chunk size (5 minutes in ms)
    CHUNK_LENGTH = 5*60*1000 # (5 minutes * 60 seconds * 1000 ms)

    # create chunks
    audio_chunks = []
    for i in range(0, len(audio), CHUNK_LENGTH):
        # get the 10 minute chunk:
        chunk = audio[i : i + CHUNK_LENGTH]
        # add 10 minute chunk to array of audio chunks
        audio_chunks.append(chunk)

    print(f"successfully split audio into {len(audio_chunks)} 5 minute segments")

    # need to transcribe each chunk using whisper:
    transcription_tasks = []
    for i, chunk in enumerate(audio_chunks):
        # --- Silence Filter ---
        # If the chunk is quieter than -40dBFS, it's likely just silence/noise.
        # Skip it to prevent Whisper from hallucinating.
        if chunk.dBFS < -40:
            print(f"Skipping chunk {i} (Silence detected)")
            continue
        # ---------------------------
        # convert pydub chunks back into raw bytes to send to CF
        buffer = io.BytesIO()
        chunk.export(buffer, format="mp3", bitrate="64k") # downsample audio to 64kbps to make file size smaller and avoid whisper breaking
        transcription_tasks.append(call_whisper(buffer.getvalue()))
    
    # we now have a list of audio chunks ready to send to the Whisper model (send all chunks asynchronously) using helper function #1
    chunk_transcripts = await asyncio.gather(*transcription_tasks)

    # join all the transcripts from chunks together
    full_transcript = " ".join(chunk_transcripts)

    # if transcript returned, pass to the LLM using helper function #2 to get in depth summary
    if full_transcript.strip():
        print("Sending chunked transcript to Llama-3...")
        notes = await call_llm(chunk_transcripts) # calling llama

        # (background) send to SQL database and Vectorize:
        asyncio.create_task(save_2_db(clean_filename, full_transcript, notes)) # send filename, transcript, and notes to DB
        asyncio.create_task(save_vectors(clean_filename, chunk_transcripts)) # send filename and chunked transcript to Vectorize

    else: # if transcript not generated (silent or inaudible mp3), return that no speech was detected
        notes = "No Speech Detected in any of the audio chunks"
    
    # return json of filename(inputted), transcript(Whisper output), and notes(Llama-3 output)
    return {
        "filename": clean_filename,
        "chunks_processed": len(audio_chunks),
        "notes": notes,
        "status": "Video processing complete, saving to DB and Vectorize in Background"
    }

# --- PDF Generation logic ---
class PDF(FPDF):
    def header(self):
        # Texas A&M Maroon Header
        self.set_font('Times', 'B', 15)
        self.set_text_color(80, 0, 0) # Aggie Maroon 
        self.cell(0, 10, 'Texas A&M University', ln=True, align='C')
        self.set_font('Times', 'I', 12)
        self.set_text_color(0, 0, 0) # Black
        self.cell(0, 10, 'Lecture Notes', ln=True, align='C')
        self.ln(5)
        # Line break
        self.set_draw_color(80, 0, 0)
        self.line(10, 30, 200, 30)
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        self.set_font('Times', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', align='C')

# POST endpoint for generating pdf
@app.post("/generate-pdf")
async def generate_pdf(data: dict, disposition: str = "attachment"): # <--- Add param
    """
    Generates PDF. 
    disposition='attachment' (Download) or 'inline' (View in browser)
    """
    filename = data.get("filename", "Lecture_Notes")
    raw_notes = data.get("notes", "")
    notes_text = clean_for_pdf(raw_notes)

    pdf = PDF()
    pdf.add_page()
    pdf.set_auto_page_break(auto=True, margin=15)

    pdf.set_font("Times", "B", 16)
    pdf.cell(0, 10, f"Lecture: {filename}", ln=True, align='L')
    pdf.ln(5)

    pdf.set_font("Helvetica", "", 12)
    lines = notes_text.replace('\r\n', '\n').split('\n')
    
    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            pdf.ln(3) 
            continue
        pdf.set_x(pdf.l_margin)
        if '**' in line:
            clean_line = line.replace("**", "").replace(":", "")
            pdf.set_font("Times", "B", 14)
            pdf.set_text_color(80, 0, 0)
            pdf.ln(3)
            pdf.multi_cell(w=pdf.epw, h=10, txt=clean_line)
            pdf.set_font("Helvetica", "", 12)
            pdf.set_text_color(0, 0, 0)
        else:
            pdf.multi_cell(w=pdf.epw, h=7, txt=line)

    pdf_bytes = pdf.output(dest='S') 

    return Response(
        content=bytes(pdf_bytes),
        media_type="application/pdf",
        # --- CHANGE HERE: Use variable disposition ---
        headers={"Content-Disposition": f"{disposition}; filename={filename}_notes.pdf"}
    )

@app.post("/process-homework")
async def process_homework(file: UploadFile = File(...)):
    print(f"Processing Homework: {file.filename}...")
    
    # get bytes from pdf
    pdf_bytes = await file.read()

    # Save homework to DB (Get the new ID)
    async with httpx.AsyncClient() as client:
        # Insert into 'homeworks' table
        await client.post(
            D1_BASE, 
            headers=HEADERS, 
            json={
                "sql": "INSERT INTO homeworks (filename) VALUES (?)",
                "params": [file.filename]
            }
        )
        # Fetch the ID (This is the standard reliable way with D1)
        id_resp = await client.post(
            D1_BASE, headers=HEADERS, json={"sql": "SELECT last_insert_rowid() as id", "params": []}
        )
        # Handle cases where D1 returns the result differently
        try:
            homework_id = id_resp.json()["result"][0]["results"][0]["id"]
        except (KeyError, IndexError):
             # Fallback if structure varies
            print("Error fetching ID, defaulting to 0")
            homework_id = 0

    extracted_problems = []

    # Convert PDF to Images using PyMuPDF 
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        print(f"Error opening PDF: {e}")
        return {"status": "error", "message": "Invalid PDF file"}

    for page_num, page in enumerate(doc):
        print(f"   --> Scanning Page {page_num + 1}...")
        
        # Render page to an image (Pixmap)
        pix = page.get_pixmap(dpi=75) # 75 DPI is good balance for Vision AI
        
        # Convert to bytes (JPEG format)
        img_bytes = pix.tobytes("jpeg")

        # 4. Send to Vision AI
        page_problems = await extract_problems_from_image(img_bytes)
        
        # 5. Link & Save each problem
        for prob_text in page_problems:
            # A. Generate Embedding
            prob_vector = await generate_embeddings([prob_text])
            
            if not prob_vector: continue

            # B. Search Vector DB for link
            search_url = VECTOR_base_url.replace("/insert", "/query")
            search_payload = {
                "vector": prob_vector[0],
                "topK": 1, 
                "returnMetadata": "all"
            }
            
            lecture_link = {"filename": None, "timestamp": None, "summary": None}
            
            async with httpx.AsyncClient() as search_client:
                s_res = await search_client.post(search_url, headers=HEADERS, json=search_payload)
                s_data = s_res.json()
                if s_data.get("result") and s_data["result"].get("matches"):
                    match = s_data["result"]["matches"][0]
                    lecture_link["filename"] = match["metadata"].get("filename")
                    lecture_link["timestamp"] = match["metadata"].get("start_time_sec")
                    lecture_link["summary"] = match["metadata"].get("text_preview")[:200]

            # C. Save to D1
            sql = """
            INSERT INTO problems (homework_id, problem_text, related_lecture_filename, related_lecture_timestamp, related_concept_summary)
            VALUES (?, ?, ?, ?, ?)
            """
            params = [
                homework_id, 
                prob_text, 
                lecture_link["filename"], 
                lecture_link["timestamp"], 
                lecture_link["summary"]
            ]
            
            async with httpx.AsyncClient() as db_client:
                await db_client.post(D1_BASE, headers=HEADERS, json={"sql": sql, "params": params})
            
            extracted_problems.append({
                "problem": prob_text[:50] + "...",
                "linked_lecture": lecture_link["filename"],
                "linked_lecture_timestamp": lecture_link["timestamp"]
            })

    return {
        "status": "success", 
        "homework_id": homework_id,
        "problems_extracted": len(extracted_problems),
        "details": extracted_problems
    }

# --- ENDPOINT: Generate Practice Exam (AI) ---
@app.post("/generate-practice-exam")
async def generate_practice_exam(request: ExamRequest):
    print(f"Generating exam for Homework ID: {request.homework_id}...")

    # 1. Fetch original problems from D1
    async with httpx.AsyncClient() as client:
        response = await client.post(
            D1_BASE,
            headers=HEADERS,
            json={
                "sql": "SELECT problem_text, related_concept_summary FROM problems WHERE homework_id = ?",
                "params": [request.homework_id]
            }
        )
        data = response.json()
        
        if not data.get("success") or not data["result"][0]["results"]:
            raise HTTPException(status_code=404, detail="No problems found for this homework ID")
            
        original_problems = data["result"][0]["results"]

    # 2. Build Prompt (Using Custom Blocks, NOT JSON)
    problems_text = "\n".join([f"Problem {i+1}: {p['problem_text']} (Context: {p['related_concept_summary']})" for i, p in enumerate(original_problems)])

    prompt = f"""
    You are a strict University Math Professor. 
    Create a 'Practice Exam' based on the following homework problems.
    
    For EACH original problem, generate a NEW problem that tests the exact same concept but changes the numbers or context.
    
    OUTPUT FORMAT:
    Do not use JSON. Use exactly this format for every problem:
    
    ---START---
    QUESTION:
    (Write the question here using LaTeX)
    
    SOLUTION:
    (Write the final answer here using LaTeX)
    
    EXPLANATION:
    (Write the step-by-step guide here)
    ---END---

    ORIGINAL PROBLEMS:
    {problems_text}
    """

    payload = {
        "messages": [
            {"role": "system", "content": "You are a helpful AI that follows custom formatting rules strictly."},
            {"role": "user", "content": prompt}
        ],
        "max_tokens": 3000
    }

    # 3. Call Llama 3.1 70B
    print("Sending to Llama 70B...")
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/@cf/meta/llama-3.1-70b-instruct",
                headers=HEADERS,
                json=payload,
                timeout=120.0
            )
            result = response.json()
            
            if "result" not in result or "response" not in result["result"]:
                 raise ValueError("Invalid response from AI model")

            raw_text = result["result"]["response"]
            
            # 4. Parse the Custom Blocks
            # This logic is 100x more robust than json.loads() for math
            exam_data = []
            
            # Split by the "---START---" delimiter
            blocks = raw_text.split("---START---")
            
            for block in blocks:
                if "---END---" not in block:
                    continue # Skip empty or malformed blocks
                
                # Extract the content before ---END---
                content = block.split("---END---")[0]
                
                # Parse fields using simple string searching
                # We use "split" with maxsplit=1 to find the headers safely
                try:
                    # Default values
                    q, s, e = "", "", ""
                    
                    # Regex is safer for flexible whitespace
                    import re
                    
                    # Find Question
                    q_match = re.search(r"QUESTION:\s*(.*?)\s*(?=SOLUTION:)", content, re.DOTALL)
                    if q_match: q = q_match.group(1).strip()
                    
                    # Find Solution
                    s_match = re.search(r"SOLUTION:\s*(.*?)\s*(?=EXPLANATION:)", content, re.DOTALL)
                    if s_match: s = s_match.group(1).strip()
                    
                    # Find Explanation (runs until end of block)
                    e_match = re.search(r"EXPLANATION:\s*(.*)", content, re.DOTALL)
                    if e_match: e = e_match.group(1).strip()
                    
                    if q and s:
                        exam_data.append({
                            "question": q,
                            "solution": s,
                            "explanation": e
                        })
                except Exception as parse_err:
                    print(f"Skipping malformed block: {parse_err}")
                    continue

            return exam_data

        except Exception as e:
            print(f"Exam Gen Error: {e}")
            raise HTTPException(status_code=500, detail=str(e))

# --- ENDPOINT: Download Exam PDF ---
class PDFRequest(BaseModel):
    title: str
    problems: List[ExamProblem]
    include_solutions: bool

@app.post("/download-exam-pdf")
async def download_exam_pdf(data: PDFRequest):
    pdf = PDF() 
    pdf.add_page()
    
    # Header
    pdf.set_font("Times", "B", 20)
    pdf.cell(0, 10, f"Practice Exam: {data.title}", ln=True, align='C')
    
    if data.include_solutions:
        pdf.set_text_color(200, 0, 0)
        pdf.set_font("Times", "I", 14)
        pdf.cell(0, 10, "INSTRUCTOR SOLUTION KEY", ln=True, align='C')
    else:
        pdf.set_font("Times", "I", 12)
        pdf.cell(0, 10, "Student Version - Time Limit: 60 mins", ln=True, align='C')
    
    pdf.ln(10)
    pdf.set_text_color(0, 0, 0)

    # Content
    for i, p in enumerate(data.problems):
        # --- QUESTION SECTION ---
        pdf.set_font("Times", "B", 14)
        pdf.cell(0, 10, f"Question {i+1}", ln=True)
        
        # Use HTML for the question text to render math
        pdf.set_font("Times", "", 12)
        q_html = f"<p>{clean_for_pdf(p.question)}</p>"
        pdf.write_html(q_html) 
        pdf.ln(5)

        # --- SOLUTION SECTION ---
        if data.include_solutions:
            # Draw a light gray box for the solution
            # Save Y position
            start_y = pdf.get_y()
            
            pdf.set_fill_color(245, 245, 245)
            # We assume a fixed height box for simplicity, or just fill background rect
            pdf.set_font("Times", "B", 12)
            pdf.set_text_color(0, 100, 0) 
            pdf.cell(0, 8, "Solution:", ln=True)
            
            pdf.set_font("Times", "", 12)
            pdf.set_text_color(0, 0, 0)
            
            # Write HTML Solution
            sol_html = f"<p><b>Answer:</b> {clean_for_pdf(p.solution)}<br><br><i>Explanation:</i> {clean_for_pdf(p.explanation)}</p>"
            pdf.write_html(sol_html)
            
            pdf.ln(10)
        else:
            # Blank space for students
            pdf.ln(40)
            pdf.set_draw_color(200, 200, 200)
            pdf.line(10, pdf.get_y(), 200, pdf.get_y())
            pdf.set_draw_color(0, 0, 0)
            pdf.ln(10)

    pdf_bytes = pdf.output(dest='S')
    
    filename = f"Practice_Exam_{'KEY' if data.include_solutions else 'Student'}.pdf"
    
    return Response(
        content=bytes(pdf_bytes),
        media_type="application/pdf",
        headers={"Content-Disposition": f"attachment; filename={filename}"}
    )