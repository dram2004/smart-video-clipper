#Backend/main.py
import os
import io
import asyncio
import httpx
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
# importss needed for chunking audio into segments
from pydub import AudioSegment
import static_ffmpeg
static_ffmpeg.add_paths() # tells pydub where to find ffmpeg binary

# --- Load Required Environment Variables ---

load_dotenv()
# get cloudflare account ID
cf_acc_id = os.getenv("CLOUDFLARE_ACCOUNT_ID")
# get cloudflare API token
cf_api_token = os.getenv("CLOUDFLARE_API_TOKEN")

# --- Verify the Environment Variables Exist
if not cf_acc_id or not cf_api_token:
    # if either of the keys do not exist, raise a runtime error
    raise RuntimeError("Missing Cloudflare credentials in .env file...")

# --- Initialize the App
app = FastAPI(title="Smart Video Clipper API")

# --- Setup CORS --> allows Next.js Frontend to communicate with our Backend ---
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], # replace with frontend URL when ready
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"]
)

# --- Setup Cloudflare Configuration ---

# base url first to use when setting up httpx asynchronous client
base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/ai/run"
# headers containing API token for asynchronous client setup
HEADERS = {"Authorization": f"Bearer {cf_api_token}"}

# --- HELPERS ---
# Helper 1: Transcribing audio using openai/whisper model
async def call_whisper(audio_bytes: bytes): # function takes bytes of audio as input
    """
    function sends raw audio bytes to cf whisper model and returns transcribed text string

    is CF works properly, response json looks like this:
    {
    "success": true,
    "result": {
        "text": "Hello world"
    }
}
    """
    # define model:
    model = "@cf/openai/whisper"

    # need to tell cf model that we are passing binary audio data to it by modifying header
    whisper_header = HEADERS.copy() # copy global header rather than modify it
    whisper_header["Content-Type"] = "application/octet-stream"
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{base_url}/{model}",
            headers=whisper_header,
            content=audio_bytes,
            timeout=60.0 # times out after 60 seconds, needed to increase for audio processing
        )

        # check if model failed to return output
        if response.status_code != 200:
            print(f"Error: {response.text}")
            # raise an HTTP exception
            raise HTTPException(status_code=response.status_code, detail="Cloudflare AI Error")
        
        # if model correctly returned something, get the json for result
        result = response.json()
        # get result from json, if result key is null return empty bracket. get text from result, if text doesnt exist return ""
        return result.get("result", {}).get("text", "") 

# Helper 2: call LLM with transcript to get a structured summary of transcript using meta/llama-3-8b-instruct model
async def call_llm(transcript: str): # function takes in string transcript
    """
    function sends transcript generated by call_whisper and calls llama-3 to generate structured study notes
    """
    # define model --> using 8b llama-3
    model = "@cf/meta/llama-3-8b-instruct"
    
    # prompt engineering:
    prompt = f"""
    You are a expert student assistant. Analyze the following lecture transcript in order to give the student in-depth notes on todays lecture.

    OUTPUT FORMAT:
    1. **Announcements**: a few bullet points going over any announcements made during the class. An example would be the professor saying 'Homework 2 is due friday and Homework 3 will be posted saturday' or 'the first exam will be next monday'
    2. **Summary**: a detailed summary of all examples/topics gone over during class. provide 1-2 sentences per each example or topic. If it is a math problem go through all the steps.
    3. **Key Points**: short and concise bullet points describing all concepts gone over during class. An example would be 'Theorem 7: Fermats Little Theorem', or 'Defining the order of a in terms of modulo 6', or 'Seven Days War'

    TRANSCRIPT:
    {transcript[:12000]}
    """
    # prepare payload: truncate transcript to ~6000 characters bc context limits of free plan
    # in future, can use "map_reduce" strategy for long transcripts
    payload = {
        "messages": [
            {"role": "system", "content": "You are a valuable study assistant"},
            {"role": "user", "content": prompt[:6000]}
        ]
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{base_url}/{model}",
            headers=HEADERS,
            json=payload,
            timeout=60.0
        )

        # make sure error didnt occur, if it did raise exception
        if response.status_code != 200:
            print(f"LLM Error: {response.text}")
            return "Error generating summary of transcript using LLM"
        
        # get response json
        result = response.json()
        return result.get("result", {}).get("response", "No Summary Generated...")

# --- ENDPOINTS ---
@app.get("/") # for base path
def health_check():
    # base path operation function
    """checks to see if server is running"""
    return {"status": "running", "service": "AI-Powered Smart Video Clipper"}

@app.post("/test-ai") # for post requests to /test-ai
async def test_ai_connection():
    # path operation function calling cf model to verify that cf credentials work
    model = "@cf/meta/llama-3-8b-instruct"
    payload = { # what is sent to the model --> just tells it to say hello as a test that it works
        "messages": [
            {"role": "user", "content": "Say 'Hello from Cloudflare' and nothing else."}
        ]
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{base_url}/{model}", 
                headers=HEADERS,
                json=payload,
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-lecture") # for post requests to /process-lecture for audio transcription + llm summary
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Upload an MP3/WAV file --> pass to Whisper --> Returns text transcript back

    Send text transcipt to LLM --> Llama-3 returns summary of lecture
    """
    # read the uploaded file into memory
    audio_data = await file.read()

    # in order to process long mp3's with whisper, need to chunk video into shorter segments
    audio = AudioSegment.from_file(io.BytesIO(audio_data)) # creates audio segment object from audio bytes

    # define chunk size (5 minutes in ms)
    CHUNK_LENGTH = 5*60*1000 # (10 minutes * 60 seconds * 1000 ms)

    # create chunks
    audio_chunks = []
    for i in range(0, len(audio), CHUNK_LENGTH):
        # get the 10 minute chunk:
        chunk = audio[i : i + CHUNK_LENGTH]
        # add 10 minute chunk to array of audio chunks
        audio_chunks.append(chunk)

    print(f"successfully split audio into {len(audio_chunks)} 5 minute segments")

    # need to transcribe each chunk using whisper:
    transcription_tasks = []
    for i, chunk in enumerate(audio_chunks):
        # convert pydub chunks back into raw bytes to send to CF
        buffer = io.BytesIO()
        chunk.export(buffer, format="mp3", bitrate="64k") # downsample audio to 64kbps to make file size smaller and avoid whisper breaking
        transcription_tasks.append(call_whisper(buffer.getvalue()))
    
    # we now have a list of audio chunks ready to send to the Whisper model (send all chunks asynchronously) using helper function #1
    chunk_transcripts = await asyncio.gather(*transcription_tasks)

    # join all the transcripts from chunks together
    full_transcript = " ".join(chunk_transcripts)

    # if transcript returned, pass to the LLM using helper function #2 to get in depth summary
    if full_transcript:
        notes = await call_llm(full_transcript)
    else: # if transcript not generated (silent or inaudible mp3), return that no speech was detected
        notes = "No Speech Detected in any of the audio chunks"
    
    # return json of filename(inputted), transcript(Whisper output), and notes(Llama-3 output)
    return {
        "filename": file.filename,
        "chunks_processed": len(audio_chunks),
        "transcript": full_transcript,
        "notes": notes
    }

