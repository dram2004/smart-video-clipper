#Backend/main.py
import os
import io
import asyncio
import uuid 
import json
import base64
import httpx
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
# imports needed for chunking audio into segments
from pydub import AudioSegment
import static_ffmpeg
static_ffmpeg.add_paths() # tells pydub where to find ffmpeg binary

# imports for PDF output logic
from fpdf import FPDF
from fastapi.responses import Response

# --- Load Required Environment Variables ---

load_dotenv()
# get cloudflare account ID
cf_acc_id = os.getenv("CLOUDFLARE_ACCOUNT_ID")
# get cloudflare API token
cf_api_token = os.getenv("CLOUDFLARE_API_TOKEN")
# get cloudflare d1 DB ID
cf_d1_ID = os.getenv("CLOUDFLARE_D1_ID")
#  get cloudflare Vector index
cf_vector_index = os.getenv("CLOUDFLARE_VECTOR_INDEX")

# --- Verify the Environment Variables Exist
if not all([cf_acc_id,cf_api_token,cf_d1_ID,cf_vector_index]):
    # if either of the keys do not exist, raise a runtime error
    raise RuntimeError("Missing Cloudflare credentials in .env file...")

# --- Initialize the App
app = FastAPI(title="Smart Video Clipper API")

# --- Setup CORS --> allows Next.js Frontend to communicate with our Backend ---
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], # replace with frontend URL when ready
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"]
)

# --- Setup Cloudflare Configuration ---

# AI base url first to use when setting up httpx asynchronous client
AI_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/ai/run"
# vector base URL
VECTOR_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/vectorize/v2/indexes/{cf_vector_index}/insert"
# D1 database base url
D1_BASE = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/d1/database/{cf_d1_ID}/query"
# headers containing API token for asynchronous client setup
HEADERS = {"Authorization": f"Bearer {cf_api_token}"}

# --- HELPERS ---
# Helper 1: Transcribing audio using openai/whisper model
# Helper 1: Transcribing audio using openai/whisper model
async def call_whisper(audio_bytes: bytes): 
    """
    Sends audio as MULTIPART FORM DATA to force 'language=en'.
    This prevents the model from crashing (Code 3010) when it hears silence/noise.
    """
    model = "@cf/openai/whisper"
    
    # We do NOT manually set Content-Type here; httpx handles the boundary automatically
    
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers={"Authorization": f"Bearer {cf_api_token}"}, 
                files={"audio_file": ("chunk.mp3", audio_bytes, "audio/mpeg")},
                data={"language": "en"}, # <--- THIS IS THE KEY FIX
                timeout=60.0 
            )

            if response.status_code != 200:
                print(f"Whisper Error (Chunk failed): {response.text}")
                return "" 
            
            result = response.json()
            return result.get("result", {}).get("text", "")

        except Exception as e:
            print(f"Whisper Exception: {e}")
            return ""

# Helper 2: call LLM with transcript to get a structured summary of transcript using meta/llama-3-8b-instruct model
async def call_llm(text_chunks: list): # function takes in string transcript
    """
    function sends transcript generated by call_whisper and calls llama-3 to generate structured study notes
    1. MAP: summarizes each five minute chunk individually
    2. REDUCE: combines all summaries into final formatted notes
    """
    # define model --> using 8b llama-3.1
    model = "@cf/meta/llama-3.1-8b-instruct"

    print(f"DEBUG: Starting Map-Reduce on {len(text_chunks)} chunks...")
    
    # STEP 1: MAP --> summarize each chunk in parallel
    async def summarize_text_chunk(text):
        if (len(text) < 50): 
            return ""
        # write prompt for single chunk processing
        prompt = f"""
        Summarize the key technical concepts and announcements in this specific lecture segment.
        Keep it concise in a range of 3-5 sentences.
        TRANSCRIPT SEGMENT:
        {text}
        """
        # define payload including role, prompt, and max number of tokens
        payload = {
            "messages": [{"role": "user", "content": prompt}],
            "max_tokens": 500
        }
        # call the model with AsyncClient
        async with httpx.AsyncClient() as client:
            try:
                response = await client.post(
                    f"{AI_base_url}/{model}", 
                    headers=HEADERS,
                    json=payload,
                    timeout=30.0
                )
                return response.json().get("result", {}).get("response", "")
            except Exception as e:
                print(f"Chunk processing error: {e}")
                return ""

    # call model on all text chunks at the same time
    mini_summaries = await asyncio.gather(*[summarize_text_chunk(chunk) for chunk in text_chunks])

    # combine summaries of all chunks into "master context" to send back to model
    combined_summaries = "\n".join(filter(None, mini_summaries)) # join all mini summaries seperated by newline

    print(f"Combined summary length from all text chunks: {len(combined_summaries)} characters")

    # STEP 2: REDUCE
    # prompt engineering:
    final_prompt = f"""
    You are an expert student assistant. 
    Below are summaries from different parts of a lecture (in chronological order).
    Compile them into one cohesive set of notes.

    OUTPUT FORMAT:
    1. **Announcements**: Any logistics/deadlines mentioned.
    2. **Summary**: A cohesive narrative of the lecture topics.
    3. **Key Points**: Bullet points of specific concepts/theorems.

    LECTURE DATA:
    {combined_summaries}
    """
    # prepare payload: truncate transcript to ~6000 characters bc context limits of free plan
    # in future, can use "map_reduce" strategy for long transcripts
    payload = {
        "messages": [
            {"role": "system", "content": "You are a valuable study assistant"},
            {"role": "user", "content": final_prompt},
        ],
        "max_tokens": 2048
    }
    # call model again with the final prompt built using the summaries of all the text chunks 
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers=HEADERS,
                json=payload,
                timeout=60.0
            )

            # --- THE DEBUG BLOCK ---
            print(f"DEBUG: Status Code: {response.status_code}")
            print(f"DEBUG: Raw Response: {response.text}") 
            # -----------------------

            # make sure error didnt occur, if it did raise exception
            if response.status_code != 200:
                print(f"LLM Error: {response.text}")
                return "Error generating summary of transcript using LLM"
            
            # get response json
            result = response.json()
            if "result" in result and "response" in result["result"]:
                return result["result"]["response"]
        
            # Fallback if AI fails
            print(f"DEBUG: Final Reduction Failed. Response: {result}")
            return combined_summaries # Return the raw summaries so the user at least gets something

        except Exception as e:
            print(f"DEBUG: Exception occurred: {e}")
            return f"Error: {str(e)}"

# Helper 3: need function to convert list of text strings into vectors for RAG
# Helper 3: Generating Embeddings (With Debugging)
async def generate_embeddings(text_chunks):
    """
    Converts list of text strings into vectors.
    """
    model = "@cf/baai/bge-base-en-v1.5"

    print(f"   --> Debug: Requesting embeddings for {len(text_chunks)} chunk(s)...")

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}",
                headers=HEADERS,
                json={"text": text_chunks}
            )
            
            # DEBUG: Print if it fails
            if response.status_code != 200:
                print(f"   ❌ Cloudflare AI Error: {response.status_code}")
                print(f"   ❌ Response Body: {response.text}")
                return []

            data = response.json()
            
            # Check if 'result' exists
            if not data.get("result"):
                print(f"   ❌ AI returned success but no result: {data}")
                return []

            return data["result"].get("data", [])

        except Exception as e:
            print(f"   ❌ Embedding Exception: {e}")
            return []

# Helper 4: need helper for generating embeddings for each chunk then saving it to vectorize with proper metadata
# Helper 4: Save vectors with Sanitized NDJSON
async def save_vectors(filename, text_chunks):
    if not cf_vector_index: return
    print("generating embeddings...")

    embeddings_data = await generate_embeddings(text_chunks)
    vectors = []
    chunk_duration_seconds = 300 

    for i, embedding_obj in enumerate(embeddings_data):
        start_time = i * chunk_duration_seconds 
        
        # Create a safe ID (spaces are okay, but let's be robust)
        safe_id = f"{filename.replace(' ', '_')}_chunk_{i}_{uuid.uuid4().hex[:6]}"
        
        # CRITICAL FIX: Remove newlines from text_preview
        raw_text = text_chunks[i]
        clean_preview = raw_text.replace("\n", " ").replace("\r", " ")[:1000] # <--- 1000 chars & No Newlines!

        vectors.append({
            "id": safe_id,
            "values": embedding_obj, 
            "metadata": {
                "filename": filename,
                "text_preview": clean_preview, 
                "chunk_index": i,
                "start_time_sec": start_time
            }
        })

    # NDJSON requires exactly one JSON object per line
    ndjson_payload = "\n".join([json.dumps(v) for v in vectors])

    async with httpx.AsyncClient() as client:
        response = await client.post(
            VECTOR_base_url,
            headers={"Authorization": f"Bearer {cf_api_token}", "Content-Type": "application/x-ndjson"},
            content=ndjson_payload,
            timeout=60.0
        )
        if response.status_code == 200:
            print(f"✅ Indexed {len(vectors)} vectors in Cloudflare!")
        else:
            print(f"❌ Vector Error: {response.text}")
# Helper 5: need helper for saving filename, transcript, and summary in database
async def save_2_db(filename, transcript, summary):
    """
    save metadata to SQL D1
    necessary for remembering all lectures passed into smart clipper
    """
    sql_command = "INSERT INTO lectures (filename, transcript, summary) VALUES (?, ?, ?)"

    # define payload with sql command and provided arguments
    payload = {"sql": sql_command, "params": [filename, transcript, summary]}

    # send to D1 using asyncclient
    async with httpx.AsyncClient() as client:
        response = await client.post(D1_BASE, headers=HEADERS, json=payload)
        
        data = response.json()
        
        if response.status_code == 200 and data.get("success"):
            print("Succesfully saved record to D1 SQL")
        else:
            print(f"❌ DATABASE ERROR: {response.status_code}")
            print(data)

# Helper 6: clean upp text to be well-structured foor pdf creation
def clean_for_pdf(text: str) -> str:
    """
    Replaces smart quotes, bullets, and other non-Latin-1 characters
    that crash the FPDF 'Times' font.
    """
    replacements = {
        "\u2018": "'", "\u2019": "'", # Single smart quotes
        "\u201c": '"', "\u201d": '"', # Double smart quotes
        "\u2022": "-",                # Bullet points
        "\u2013": "-", "\u2014": "-", # En-dashes and Em-dashes
        "\u2026": "...",              # Ellipsis
    }
    for char, replacement in replacements.items():
        text = text.replace(char, replacement)
    
    # Finally, force convert to Latin-1, replacing any other unknowns with '?'
    return text.encode('latin-1', 'replace').decode('latin-1')

# --- ENDPOINTS ---
@app.get("/") # for base path
def health_check():
    # base path operation function
    """checks to see if server is running"""
    return {"status": "running", "service": "AI-Powered Smart Video Clipper"}

@app.get("/lectures") # endpoint for frontend to call to query database on which lectures are currently stored
async def get_lectures():
    """
    Path operation function responsible for fetching all saved lectures from D1 DB
    """
    # write SQL query
    sql = "SELECT * FROM lectures ORDER BY upload_date DESC" # gets all lectures, ordered in descending order based on the date they got uploaded

    # query d1 using httpx.AsyncClient
    async with httpx.AsyncClient() as client:
        response = await client.post(
            D1_BASE,
            headers=HEADERS,
            json={"sql": sql, "params": []}
        )
    
        # parse D1 response structure
        data = response.json() # get json received from D1
        if data.get("success") and data.get("result"):
            # if d1 returned success and result, return query rows
            return data["result"][0].get("results", [])
        # if did not get successful response, return nothing
        return []
 
@app.get("/search")
async def search_lectures(q: str):
    if not q: return []
    print(f"\n--- DEBUG SEARCH START: '{q}' ---")

    # 1. Generate Embedding
    print("Step 1: Generating embedding...")
    try:
        query_vector_data = await generate_embeddings([q])
    except Exception as e:
        print(f"❌ Error generating embedding: {e}")
        return []

    if not query_vector_data: 
        print("❌ Embedding generation returned empty data!")
        return []
    
    query_vector = query_vector_data[0]
    print(f"✅ Embedding generated (Length: {len(query_vector)})")

    # 2. Prepare Vectorize Request
    search_url = VECTOR_base_url.replace("/insert", "/query")
    payload = {
        "vector": query_vector,
        "topK": 5, 
        "returnMetadata": "all"
    }
    
    print(f"Step 2: Sending query to Cloudflare Vectorize...")
    print(f"Target URL: {search_url}")

    # 3. Send Request
    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(search_url, headers=HEADERS, json=payload)
            
            # Print raw response details
            print(f"Step 3: Received Response. Status Code: {response.status_code}")
            print(f"Raw Response Body: {response.text}")

            data = response.json()
            
            matches = []
            if data.get("result"):
                raw_matches = data["result"].get("matches", [])
                print(f"✅ Found {len(raw_matches)} raw matches from Cloudflare.")
                
                for match in raw_matches:
                    score = match.get("score", 0)
                    filename = match.get("metadata", {}).get("filename", "Unknown")
                    print(f" -- Candidate: {filename} | Score: {score}")

                    matches.append({
                        "score": score,
                        "text": match["metadata"].get("text_preview", "No text"),
                        "filename": filename,
                        "timestamp": match["metadata"].get("start_time_sec", 0)
                    })
            else:
                print("⚠️ Cloudflare returned 'success' but no 'result' object.")

            return matches

        except Exception as e:
            print(f"❌ Search Exception: {e}")
            return []

@app.post("/test-ai") # for post requests to /test-ai
async def test_ai_connection():
    # path operation function calling cf model to verify that cf credentials work
    model = "@cf/meta/llama-3-8b-instruct"
    payload = { # what is sent to the model --> just tells it to say hello as a test that it works
        "messages": [
            {"role": "user", "content": "Say 'Hello from Cloudflare' and nothing else."}
        ]
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}", 
                headers=HEADERS,
                json=payload,
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-lecture") # for post requests to /process-lecture for audio transcription + llm summary
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Upload an MP3/WAV file --> pass to Whisper --> Returns text transcript back

    Send text transcipt to LLM --> Llama-3 returns summary of lecture
    """
    print(f"Processing {file.filename}...")

    clean_filename = os.path.splitext(file.filename)[0]

    # read the uploaded file into memory
    audio_data = await file.read()

    # in order to process long mp3's with whisper, need to chunk video into shorter segments
    audio = AudioSegment.from_file(io.BytesIO(audio_data)) # creates audio segment object from audio bytes

    # define chunk size (5 minutes in ms)
    CHUNK_LENGTH = 5*60*1000 # (5 minutes * 60 seconds * 1000 ms)

    # create chunks
    audio_chunks = []
    for i in range(0, len(audio), CHUNK_LENGTH):
        # get the 10 minute chunk:
        chunk = audio[i : i + CHUNK_LENGTH]
        # add 10 minute chunk to array of audio chunks
        audio_chunks.append(chunk)

    print(f"successfully split audio into {len(audio_chunks)} 5 minute segments")

    # need to transcribe each chunk using whisper:
    transcription_tasks = []
    for i, chunk in enumerate(audio_chunks):
        # --- Silence Filter ---
        # If the chunk is quieter than -40dBFS, it's likely just silence/noise.
        # Skip it to prevent Whisper from hallucinating.
        if chunk.dBFS < -40:
            print(f"Skipping chunk {i} (Silence detected)")
            continue
        # ---------------------------
        # convert pydub chunks back into raw bytes to send to CF
        buffer = io.BytesIO()
        chunk.export(buffer, format="mp3", bitrate="64k") # downsample audio to 64kbps to make file size smaller and avoid whisper breaking
        transcription_tasks.append(call_whisper(buffer.getvalue()))
    
    # we now have a list of audio chunks ready to send to the Whisper model (send all chunks asynchronously) using helper function #1
    chunk_transcripts = await asyncio.gather(*transcription_tasks)

    # join all the transcripts from chunks together
    full_transcript = " ".join(chunk_transcripts)

    # if transcript returned, pass to the LLM using helper function #2 to get in depth summary
    if full_transcript.strip():
        print("Sending chunked transcript to Llama-3...")
        notes = await call_llm(chunk_transcripts) # calling llama

        # (background) send to SQL database and Vectorize:
        asyncio.create_task(save_2_db(clean_filename, full_transcript, notes)) # send filename, transcript, and notes to DB
        asyncio.create_task(save_vectors(clean_filename, chunk_transcripts)) # send filename and chunked transcript to Vectorize

    else: # if transcript not generated (silent or inaudible mp3), return that no speech was detected
        notes = "No Speech Detected in any of the audio chunks"
    
    # return json of filename(inputted), transcript(Whisper output), and notes(Llama-3 output)
    return {
        "filename": clean_filename,
        "chunks_processed": len(audio_chunks),
        "notes": notes,
        "status": "Video processing complete, saving to DB and Vectorize in Background"
    }

# --- PDF Generation logic ---
class PDF(FPDF):
    def header(self):
        # Texas A&M Maroon Header
        self.set_font('Times', 'B', 15)
        self.set_text_color(80, 0, 0) # Aggie Maroon 
        self.cell(0, 10, 'Texas A&M University', ln=True, align='C')
        self.set_font('Times', 'I', 12)
        self.set_text_color(0, 0, 0) # Black
        self.cell(0, 10, 'Lecture Notes', ln=True, align='C')
        self.ln(5)
        # Line break
        self.set_draw_color(80, 0, 0)
        self.line(10, 30, 200, 30)
        self.ln(5)

    def footer(self):
        self.set_y(-15)
        self.set_font('Times', 'I', 8)
        self.cell(0, 10, f'Page {self.page_no()}', align='C')

# POST endpoint for generating pdf
@app.post("/generate-pdf")
async def generate_pdf(data: dict, disposition: str = "attachment"): # <--- Add param
    """
    Generates PDF. 
    disposition='attachment' (Download) or 'inline' (View in browser)
    """
    filename = data.get("filename", "Lecture_Notes")
    raw_notes = data.get("notes", "")
    notes_text = clean_for_pdf(raw_notes)

    pdf = PDF()
    pdf.add_page()
    pdf.set_auto_page_break(auto=True, margin=15)

    pdf.set_font("Times", "B", 16)
    pdf.cell(0, 10, f"Lecture: {filename}", ln=True, align='L')
    pdf.ln(5)

    pdf.set_font("Helvetica", "", 12)
    lines = notes_text.replace('\r\n', '\n').split('\n')
    
    for line in lines:
        stripped_line = line.strip()
        if not stripped_line:
            pdf.ln(3) 
            continue
        pdf.set_x(pdf.l_margin)
        if '**' in line:
            clean_line = line.replace("**", "").replace(":", "")
            pdf.set_font("Times", "B", 14)
            pdf.set_text_color(80, 0, 0)
            pdf.ln(3)
            pdf.multi_cell(w=pdf.epw, h=10, txt=clean_line)
            pdf.set_font("Helvetica", "", 12)
            pdf.set_text_color(0, 0, 0)
        else:
            pdf.multi_cell(w=pdf.epw, h=7, txt=line)

    pdf_bytes = pdf.output(dest='S') 

    return Response(
        content=bytes(pdf_bytes),
        media_type="application/pdf",
        # --- CHANGE HERE: Use variable disposition ---
        headers={"Content-Disposition": f"{disposition}; filename={filename}_notes.pdf"}
    )