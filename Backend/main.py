#Backend/main.py
import os
import io
import asyncio
import uuid 
import json
import httpx
from fastapi import FastAPI, UploadFile, File, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from dotenv import load_dotenv
# importss needed for chunking audio into segments
from pydub import AudioSegment
import static_ffmpeg
static_ffmpeg.add_paths() # tells pydub where to find ffmpeg binary

# --- Load Required Environment Variables ---

load_dotenv()
# get cloudflare account ID
cf_acc_id = os.getenv("CLOUDFLARE_ACCOUNT_ID")
# get cloudflare API token
cf_api_token = os.getenv("CLOUDFLARE_API_TOKEN")
# get cloudflare d1 DB ID
cf_d1_ID = os.getenv("CLOUDFLARE_D1_ID")
#  get cloudflare Vector index
cf_vector_index = os.getenv("CLOUDFLARE_VECTOR_INDEX")

# --- Verify the Environment Variables Exist
if not all([cf_acc_id,cf_api_token,cf_d1_ID,cf_vector_index]):
    # if either of the keys do not exist, raise a runtime error
    raise RuntimeError("Missing Cloudflare credentials in .env file...")

# --- Initialize the App
app = FastAPI(title="Smart Video Clipper API")

# --- Setup CORS --> allows Next.js Frontend to communicate with our Backend ---
app.add_middleware(
    CORSMiddleware, 
    allow_origins=["*"], # replace with frontend URL when ready
    allow_credentials=True, 
    allow_methods=["*"],
    allow_headers=["*"]
)

# --- Setup Cloudflare Configuration ---

# AI base url first to use when setting up httpx asynchronous client
AI_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/ai/run"
# vector base URL
VECTOR_base_url = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/vectorize/v2/indexes/{cf_vector_index}/insert"
# D1 database base url
D1_BASE = f"https://api.cloudflare.com/client/v4/accounts/{cf_acc_id}/d1/database/{cf_d1_ID}/query"
# headers containing API token for asynchronous client setup
HEADERS = {"Authorization": f"Bearer {cf_api_token}"}

# --- HELPERS ---
# Helper 1: Transcribing audio using openai/whisper model
async def call_whisper(audio_bytes: bytes): # function takes bytes of audio as input
    """
    function sends raw audio bytes to cf whisper model and returns transcribed text string

    is CF works properly, response json looks like this:
    {
    "success": true,
    "result": {
        "text": "Hello world"
    }
}
    """
    # define model:
    model = "@cf/openai/whisper"

    # need to tell cf model that we are passing binary audio data to it by modifying header
    whisper_header = HEADERS.copy() # copy global header rather than modify it
    whisper_header["Content-Type"] = "application/octet-stream"
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{AI_base_url}/{model}",
            headers=whisper_header,
            content=audio_bytes,
            timeout=60.0 # times out after 60 seconds, needed to increase for audio processing
        )

        # check if model failed to return output
        if response.status_code != 200:
            print(f"Error: {response.text}")
            # raise an HTTP exception
            raise HTTPException(status_code=response.status_code, detail="Cloudflare AI Error")
        
        # if model correctly returned something, get the json for result
        result = response.json()
        # get result from json, if result key is null return empty bracket. get text from result, if text doesnt exist return ""
        return result.get("result", {}).get("text", "") 

# Helper 2: call LLM with transcript to get a structured summary of transcript using meta/llama-3-8b-instruct model
async def call_llm(transcript: str): # function takes in string transcript
    """
    function sends transcript generated by call_whisper and calls llama-3 to generate structured study notes
    """
    # define model --> using 8b llama-3
    model = "@cf/meta/llama-3-8b-instruct"
    
    # prompt engineering:
    prompt = f"""
    You are a expert student assistant. Analyze the following lecture transcript in order to give the student in-depth notes on todays lecture.

    OUTPUT FORMAT:
    1. **Announcements**: a few bullet points going over any announcements made during the class. An example would be the professor saying 'Homework 2 is due friday and Homework 3 will be posted saturday' or 'the first exam will be next monday'
    2. **Summary**: a detailed summary of all examples/topics gone over during class. provide 1-2 sentences per each example or topic. If it is a math problem go through all the steps.
    3. **Key Points**: short and concise bullet points describing all concepts gone over during class. An example would be 'Theorem 7: Fermats Little Theorem', or 'Defining the order of a in terms of modulo 6', or 'Seven Days War'

    TRANSCRIPT:
    {transcript[:12000]}
    """
    # prepare payload: truncate transcript to ~6000 characters bc context limits of free plan
    # in future, can use "map_reduce" strategy for long transcripts
    payload = {
        "messages": [
            {"role": "system", "content": "You are a valuable study assistant"},
            {"role": "user", "content": prompt},
        ],
        "max_tokens": 2048
    }

    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{AI_base_url}/{model}",
            headers=HEADERS,
            json=payload,
            timeout=60.0
        )

        # make sure error didnt occur, if it did raise exception
        if response.status_code != 200:
            print(f"LLM Error: {response.text}")
            return "Error generating summary of transcript using LLM"
        
        # get response json
        result = response.json()
        return result.get("result", {}).get("response", "No Summary Generated...")

# Helper 3: need function to convert list of text strings into vectors for RAG
async def generate_embeddings(text_chunks):
    """
    converts list of text strings into vectors
    """
    model = "@cf/baai/bge-base-en-v1.5"

    # use AsyncClient to send text_chunks to model to generate text embeddings
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{AI_base_url}/{model}",
            headers=HEADERS,
            json={"text": text_chunks}
        )
    return response.json().get("result", {}).get("data", [])

# Helper 4: need helper for generating embeddings for each chunk then saving it to vectorize with proper metadata
async def save_vectors(filename, text_chunks):
    """
    1. Generate embeddings for each chunk.
    2. Save to Vectorize with metadata (timestamp).
    """
    # make sure we have a index name before continuing
    if not cf_vector_index:
        print("skipping vector save because no index has been configured")
        return
    print("generating embeddings...")

    # call CF bge model to generate embeddings
    embeddings_data = await generate_embeddings(text_chunks) # each chunks embedding is list of 768 numbers representing meaning

    # need list of vector JSONs
    vectors = []

    # define the length each chunk represents
    chunk_duration_seconds = 300 # 5-minute chunks

    for i, embedding_obj in enumerate(embeddings_data):
        # calculate vectors start time for metadata
        start_time = i * chunk_duration_seconds 
        # make vector id
        vector_id = f"{filename}_chunk_{i}_{uuid.uuid4().hex[:6]}"
        # append vector to vectors in JSON form
        vectors.append({
            "id": vector_id,
            "values": embedding_obj, # list of 768 numbers
            # add metatdata (filename, text preview, chunk index, and start time)
            "metadata": {
                "filename": filename,
                "text_preview": text_chunks[i][:100], # preview is first 100 characters of text chunk,
                "chunk_index": i,
                "start_time_sec": start_time
            }
        })

    # by now we have a list of vector JSONs representing each processed chunk
    # now insert all into Vectorize using NDJSON format 
    ndjson_payload = "\n".join([json.dumps(v) for v in vectors])

    # send ndjson_payload to Vectorize in CF
    async with httpx.AsyncClient() as client:
        response = await client.post(
            VECTOR_base_url,
            headers={"Authorization": f"Bearer {cf_api_token}", "Content-Type": "application/x-ndjson"}, # specify input format
            content=ndjson_payload,
            timeout=60.0
        )

        # make sure we successfully sent to Vectorize:
        if response.status_code == 200:
            print(f"Indexed {len(vectors)} vectors in Cloudflare!")
        else:
            print(f"Vector Error: {response.text}")

# Helper 5: need helper for saving filename, transcript, and summary in database
async def save_2_db(filename, transcript, summary):
    """
    save metadata to SQL D1
    necessary for remembering all lectures passed into smart clipper
    """
    sql_command = "INSERT INTO lectures (filename, transcript, summary) VALUES (?, ?, ?)"

    # define payload with sql command and provided arguments
    payload = {"sql": sql_command, "params": [filename, transcript, summary]}

    # send to D1 using asyncclient
    async with httpx.AsyncClient() as client:
        response = await client.post(D1_BASE, headers=HEADERS, json=payload)
        
        data = response.json()
        
        if response.status_code == 200 and data.get("success"):
            print("Succesfully saved record to D1 SQL")
        else:
            print(f"âŒ DATABASE ERROR: {response.status_code}")
            print(data)

# --- ENDPOINTS ---
@app.get("/") # for base path
def health_check():
    # base path operation function
    """checks to see if server is running"""
    return {"status": "running", "service": "AI-Powered Smart Video Clipper"}

@app.post("/test-ai") # for post requests to /test-ai
async def test_ai_connection():
    # path operation function calling cf model to verify that cf credentials work
    model = "@cf/meta/llama-3-8b-instruct"
    payload = { # what is sent to the model --> just tells it to say hello as a test that it works
        "messages": [
            {"role": "user", "content": "Say 'Hello from Cloudflare' and nothing else."}
        ]
    }

    async with httpx.AsyncClient() as client:
        try:
            response = await client.post(
                f"{AI_base_url}/{model}", 
                headers=HEADERS,
                json=payload,
                timeout=10.0
            )
            response.raise_for_status()
            return response.json()
        
        except Exception as e:
            raise HTTPException(status_code=500, detail=str(e))

@app.post("/process-lecture") # for post requests to /process-lecture for audio transcription + llm summary
async def transcribe_audio(file: UploadFile = File(...)):
    """
    Upload an MP3/WAV file --> pass to Whisper --> Returns text transcript back

    Send text transcipt to LLM --> Llama-3 returns summary of lecture
    """
    print(f"Processing {file.filename}...")

    # read the uploaded file into memory
    audio_data = await file.read()

    # in order to process long mp3's with whisper, need to chunk video into shorter segments
    audio = AudioSegment.from_file(io.BytesIO(audio_data)) # creates audio segment object from audio bytes

    # define chunk size (5 minutes in ms)
    CHUNK_LENGTH = 5*60*1000 # (10 minutes * 60 seconds * 1000 ms)

    # create chunks
    audio_chunks = []
    for i in range(0, len(audio), CHUNK_LENGTH):
        # get the 10 minute chunk:
        chunk = audio[i : i + CHUNK_LENGTH]
        # add 10 minute chunk to array of audio chunks
        audio_chunks.append(chunk)

    print(f"successfully split audio into {len(audio_chunks)} 5 minute segments")

    # need to transcribe each chunk using whisper:
    transcription_tasks = []
    for i, chunk in enumerate(audio_chunks):
        # convert pydub chunks back into raw bytes to send to CF
        buffer = io.BytesIO()
        chunk.export(buffer, format="mp3", bitrate="64k") # downsample audio to 64kbps to make file size smaller and avoid whisper breaking
        transcription_tasks.append(call_whisper(buffer.getvalue()))
    
    # we now have a list of audio chunks ready to send to the Whisper model (send all chunks asynchronously) using helper function #1
    chunk_transcripts = await asyncio.gather(*transcription_tasks)

    # join all the transcripts from chunks together
    full_transcript = " ".join(chunk_transcripts)

    # if transcript returned, pass to the LLM using helper function #2 to get in depth summary
    if full_transcript:
        print("Sending transcript to Llama-3...")
        notes = await call_llm(full_transcript) # calling llama

        # (background) send to SQL database and Vectorize:
        asyncio.create_task(save_2_db(file.filename, full_transcript, notes)) # send filename, transcript, and notes to DB
        asyncio.create_task(save_vectors(file.filename, chunk_transcripts)) # send filename and chunked transcript to Vectorize

    else: # if transcript not generated (silent or inaudible mp3), return that no speech was detected
        notes = "No Speech Detected in any of the audio chunks"
    
    # return json of filename(inputted), transcript(Whisper output), and notes(Llama-3 output)
    return {
        "filename": file.filename,
        "chunks_processed": len(audio_chunks),
        "notes": notes,
        "status": "Video processing complete, saving to DB and Vectorize in Background"
    }

